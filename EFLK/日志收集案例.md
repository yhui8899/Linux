# 															日志收集案例

首先filebeat收集日志存到redis中，logstash去redis中提取日志过滤然后存到ES集群中；

### filebeat收集nginx日志：

#### 1、首先将nginx日志设置为json格式：

```
log_format json '{ "@timestamp": "$time_iso8601", '
         '"remote_addr": "$remote_addr", '
         '"remote_user": "$remote_user", '
         '"body_bytes_sent": "$body_bytes_sent", '
         '"request_time": "$request_time", '
         '"status": "$status", '
         '"request_uri": "$request_uri", '
         '"request_method": "$request_method", '
         '"http_referrer": "$http_referer", '
         '"http_x_forwarded_for": "$http_x_forwarded_for", '
         '"http_user_agent": "$http_user_agent"}';
```

#### 编写filebeat.yml配置文件：

```
filebeat.prospectors:
- type: log      #收集nginx访问日志;
  paths:
    - /usr/local/nginx/logs/access.log		#nginx日志路径；
  # tags: ["access"]			#tags定义，这里先不用
  fields:						#新增字段
    app: www					#app,可以定义为项目名称，如：app: apollo
    type: nginx-access			#日志类型：nginx-access 访问日志
  fields_under_root: true		#开启后，上面的app和type字段将被视为与fields同级处理；	
 
 #收集nginx错误日志，收集方式和nginx访问日志一样；
- type: log			
  paths:
    - /usr/local/nginx/logs/error.log
  # ags: ["error"]
  fields:
    app: www
    type: nginx-error
  fields_under_root: true
  
#将日志写到redis中
output.redis:
  hosts: ["192.168.83.128"]			#redis主机IP地址
  password: "123456"				#redis验证密码
  key: "filebeat"					#将数据写到redis的filebeat的key中
  db: 0								#数据库，默认为0
  datatype: list					#数据类型，默认为列表：list	
```

##### 启动filebeat：

```
nohup /usr/local/filebeat-6.2.1/filebeat &
```

##### 可以查看filebeat的日志文件是否有监听日志：

tail -f  /usr/local/filebeat-6.2.1/logs/filebeat

```
2020-03-03T14:33:07.623+0800    INFO    crawler/crawler.go:48   Loading Prospectors: 3
2020-03-03T14:33:07.624+0800    INFO    [monitoring]    log/log.go:97   Starting metrics logging every 30s
2020-03-03T14:33:07.625+0800    INFO    log/prospector.go:111   Configured paths: [/usr/local/nginx/logs/access.log]
#以上信息可以看出已经监控了nginx访问日志
```

##### 查看redis数据：

redis-cli -a 123456    登录redis；

```
127.0.0.1:6379> keys *
1) "filebeat"			#这个就是filebeat存放日志的key;
127.0.0.1:6379> llen filebeat
(integer) 94			#看到已有94条数据；
127.0.0.1:6379> 
```



#### logstash配置文件：

```
input {					#从redis中消费数据
    redis {
        host => "192.168.83.128"			#redis主机IP
        port => 6379						#redis端口
        password => "123456"				#redis验证密码	
        db => "0"							#数据库，默认为0	
        data_type => "list"					#数据类型，默认为列表：list
        key => "filebeat"					#指定redis存储日志的key；
    }
}

#filter日志过滤：
filter {
  if [app] == "www" {   				 #判断app等于www就往下执行
    if [type] == "nginx-access" { #判断类型等于nginx-access就往下执行，根据filebeat的type来判断
      json {							 #使用json模块来解析
          source => "message"			 #对message字段进行解析，会保留两个数据，一个是message的源数据，另一个是解析后的数据字段，我们只要保留解析后的数据字段即可；
          remove_field => ["message"]	#删除message的源数据；
      }
      geoip {						#使用geoip插件进行过滤
          source => "remote_addr"	#使用nginx的remote_addr即IP地址来做地理位置解析
          target => "geoip"			#将上面的IP地理位置解析结果放到这里
          database => "/opt/GeoLite2-City_20200225/GeoLite2-City.mmdb" #IP地址地理位置数据库
          add_field => ["[geoip][coordinates]", "%{[geoip][longitude]}"] #添加字段
          add_field => ["[geoip][coordinates]", "%{[geoip][latitude]}"]  #添加字段
      }
      mutate {		#使用修改的的插件，mutate 变异的意思，可以修改某个字段中的数据，替换或改变
          convert => ["[geoip][coordinates]", "float"]  
      }
    }
  }
}

#日志输出到ES集群；
output {
  elasticsearch {
      hosts  => ["http://192.168.0.212:9200","http://192.168.0.213:9200","http://192.168.0.214:9200"]
      index  => "logstash-%{type}-%{+YYYY.MM.dd}"	#{type}引用了filebeat中的type,会根据这个字段自动的ES中建立索引，会建立nginx-access和nginx-error；
  }
  stdout{codec => rubydebug } #标准输出到控制台；
  }
```

-------------------------



### filebeat收集java堆栈日志：

#### 首先部署一个tomcat

下载tomcat软件包：apache-tomcat-8.0.50.tar.gz

```
tar -xf apache-tomcat-8.0.50.tar.gz

mv apache-tomcat-8.0.50  /usr/local/
```

##### 启动tomcat：

```
/usr/local/apache-tomcat-8.0.50/bin/startup.sh
```

##### 编写filebeat日志收集配置文件：

```
filebeat.prospectors:
- type: log			#日志类型
  paths:
    - /usr/local/apache-tomcat-8.0.50/logs/catalina.out		#日志文件路径
  # tags: ["tomcat"]			#tags标签
 
 fields:						#新增的字段，app和type
    app: www					#app为新增字段，www可以作为项目名称来定义，例如：Apollo
    type: tomcat-catalina		#type为新增字段，tomcat-catalina 表示为tomcat的日志；
  fields_under_root: true	#开启后，上面的app和type字段将被视为与fields同级处理，否则无法识别这两个新增的字段；
  multiline:			#multiline，多行的意思，多行匹配
    pattern: '^\['		#pattern，模式的意思，使用正则匹配，以[中括号开头的
    negate: true		#negate，否的意思，开启true 表示不是以[中括号开头的就合并为上一行；
    match: after		#match，匹配的意思，#after 或 before，合并到上一行的末尾或开头

#输出到redis中；
output.redis:
  hosts: ["192.168.83.128"]
  password: "123456"
  key: "filebeat"
  db: 0
  datatype: list
  
-------------------------------------
#匹配注解：

multiline.pattern: '^\['
## 多行合并参数，正则表达式

multiline.negate: true
## true 或 false；默认是false，匹配pattern的行合并到上一行；true，不匹配pattern的行合并到上一行

multiline.match: after
## after 或 before，合并到上一行的末尾或开头

参考：https://blog.51cto.com/tchuairen/2166620
  
```

--------------------------------------------



### 定制日志格式收集

定制日志格式：指的是logstash的json或者其他模块无法解析的，没有太多规律性的日志，因此称为定制日志格式

例如：nginx的访问日志，因此需要使用grok来匹配

```
nginx日志：

192.168.83.1 - admin [03/Mar/2020:14:50:37 +0800] "POST /elasticsearch/_msearch HTTP/1.1" 200 13370 "http://192.168.83.128/app/kibana" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0" "-"

使用grok来匹配一下：
%{IPV4:remote_addr} - (%{USERNAME:remote_user}|-) \[%{HTTPDATE:time_local}\] \"%{WORD:request_method} %{URIPATHPARAM:request_uri} HTTP/%{NUMBER:http_protocol}\" %{NUMBER:http_status} %{NUMBER:body_bytes_sent} \"%{GREEDYDATA:http_referer}\" \"%{GREEDYDATA:http_user_agent}\" \"(%{IPV4:http_x_forwarded_for}|-)\"

注解：
%{grok正则匹配变量:nginx日志格式的内置变量} 	#固定格式；
1、%{IPV4:remote_addr}: #匹配IP地址，IPV4是grok的正则匹配，remote_addr是nginx里面的日志格式的内置变量，$remote_addr

2、(%{USERNAME:remote_user}|-)	#匹配用户，如果匹配不到就以-代替，需要使用（%{正则匹配:nginx变量}|-）的格式

3、\[%{HTTPDATE:time_local}\]	#匹配时间，需要使用[]中括号括起来，中括号是nginx日志里面的固定字符所以需要使用\进行转义；

4、\"%{WORD:request_method} %{URIPATHPARAM:request_uri} HTTP/%{NUMBER:http_protocol}\" #匹配uri和协议，双引号是nginx日志里面的固定字符所以需要使用\进行转义

6、%{NUMBER:http_status} %{NUMBER:body_bytes_sent}	#匹配状态码和文件的大小；

7、\"%{GREEDYDATA:http_referer}\" \"%{GREEDYDATA:http_user_agent}\"  #匹配http_referer来源和用户使用的客户端；

8、\"(%{IPV4:http_x_forwarded_for}|-)\"   #匹配forwarded_for，双引号是nginx日志里面的固定字符所以需要使用\进行转义

注意：建议先到：ttp://grok.ctnrs.com/调试网站进行匹配，input是写入日志，pattern 正则匹配调试；
```

##### logstash过滤nginx日志配置文件代码如下：

```
vim logstash_nginx_to_es.conf
input {
    redis {
        host => "192.168.83.128"
        port => 6379
        password => "123456"
        db => "0"
        data_type => "list"
        key => "filebeat"
    }
}

#filter过滤
filter {
  if [app] == "www" {
    if [type] == "nginx-access" {
      grok {
        match => {
          "message" => "%{IPV4:remote_addr} - (%{USERNAME:remote_user}|-) \[%{HTTPDATE:time_local}\] \"%{WORD:request_method} %{URIPATHPARAM:request_uri} HTTP/%{NUMBER:http_protocol}\" %{NUMBER:http_status} %{NUMBER:body_bytes_sent} \"%{GREEDYDATA:http_referer}\" \"%{GREEDYDATA:http_user_agent}\" \"(%{IPV4:http_x_forwarded_for}|-)\""			#匹配nginx的日志格式
        }
        overwrite => ["message"]		#重写message信息
      }
      geoip {						#IP地理位置解析
          source => "remote_addr"		#source使用nginx日志的远程IP地址来解析
          target => "geoip"				#将上面的IP地理位置解析结果放到这里
          database => "/opt/GeoLite2-City_20200225/GeoLite2-City.mmdb" #IP地址地理位置数据库
          add_field => ["[geoip][coordinates]", "%{[geoip][longitude]}"]  #添加字段
          add_field => ["[geoip][coordinates]", "%{[geoip][latitude]}"]   #添加字段
      }
      date {  #日志时间过滤器用于从字段中解析日期，然后使用日期或时间戳作为事件的logstash时间戳。
          locale => "en"
          match => ["time_local", "dd/MMM/yyyy:HH:mm:ss Z"]  #将日志格式里面的time_local与dd/MMM/yyyy:HH:mm:ss Z进行匹配，匹配成功就覆盖默认的时间戳，默认覆盖@timestamp时间戳；其实就是nginx日志中的访问时间；覆盖后会写到logstash的时间戳上，因为logstash默认时间戳会慢8小时；
      }
      mutate {		#使用修改的的插件，mutate 变异的意思，可以修改某个字段中的数据，替换或改变
          convert => ["[geoip][coordinates]", "float"]  
      }
    }
  }
}

#输出到ES集群
output {
  elasticsearch {
      hosts  => ["http://192.168.0.212:9200","http://192.168.0.213:9200","http://192.168.0.214:9200"]
      index  => "logstash-%{type}-%{+YYYY.MM.dd}"
  }
  stdout{codec => rubydebug }		#标准输出到控制台，调试用；
}
```

##### 启动logstash：

```
/usr/local/logstash-6.2.4/bin/logstash -f  /usr/local/logstash-6.2.4/conf.d/logstash_nginx_to_es.conf
```

